{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Event Explorer\n",
    "\n",
    "Browse and analyze captured WebSocket events from Rugs.fun sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _paths import *\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load Discovered Events\n",
    "\n",
    "Load schemas and fields from the knowledge pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = load_discovered_schemas()\n",
    "fields = load_discovered_fields()\n",
    "\n",
    "print(f\"Total events discovered: {len(schemas)}\")\n",
    "print(f\"Total field paths: {len(fields)}\")\n",
    "\n",
    "# Create event summary\n",
    "event_summary = []\n",
    "for event_name, schema in schemas.items():\n",
    "    event_summary.append({\n",
    "        'event': event_name,\n",
    "        'field_count': len(schema.get('fields', {})),\n",
    "        'sample_count': schema.get('sample_count', 0),\n",
    "        'tier': schema.get('tier', 'OBSERVED')\n",
    "    })\n",
    "\n",
    "df_events = pd.DataFrame(event_summary)\n",
    "if not df_events.empty:\n",
    "    df_events = df_events.sort_values('sample_count', ascending=False)\n",
    "    display(df_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Event Type Filter\n",
    "\n",
    "Filter events by tier (OBSERVED, VERIFIED, CANONICAL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by tier\n",
    "TIER = \"OBSERVED\"  # Change to VERIFIED or CANONICAL\n",
    "\n",
    "filtered = {k: v for k, v in schemas.items() if v.get('tier', 'OBSERVED') == TIER}\n",
    "print(f\"Events with tier '{TIER}': {len(filtered)}\")\n",
    "\n",
    "for event_name in list(filtered.keys())[:10]:\n",
    "    print(f\"  - {event_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Event Detail Inspector\n",
    "\n",
    "Inspect a specific event's schema and sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select event to inspect\n",
    "EVENT_NAME = \"gameStateUpdate\"  # Change to inspect other events\n",
    "\n",
    "if EVENT_NAME in schemas:\n",
    "    event_schema = schemas[EVENT_NAME]\n",
    "    print(f\"Event: {EVENT_NAME}\")\n",
    "    print(f\"Tier: {event_schema.get('tier', 'OBSERVED')}\")\n",
    "    print(f\"Sample count: {event_schema.get('sample_count', 0)}\")\n",
    "    print(f\"\\nFields ({len(event_schema.get('fields', {}))}):\\n\")\n",
    "    \n",
    "    for field_path, field_info in event_schema.get('fields', {}).items():\n",
    "        field_type = field_info.get('type', 'unknown')\n",
    "        examples = field_info.get('examples', [])\n",
    "        print(f\"  {field_path}: {field_type}\")\n",
    "        if examples:\n",
    "            print(f\"    examples: {examples[:3]}\")\n",
    "else:\n",
    "    print(f\"Event '{EVENT_NAME}' not found.\")\n",
    "    print(f\"Available events: {list(schemas.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Field Search\n",
    "\n",
    "Search for fields across all events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for fields containing a pattern\n",
    "SEARCH_PATTERN = \"price\"  # Change to search for other fields\n",
    "\n",
    "matches = []\n",
    "for field_path, field_info in fields.items():\n",
    "    if SEARCH_PATTERN.lower() in field_path.lower():\n",
    "        matches.append({\n",
    "            'field': field_path,\n",
    "            'type': field_info.get('type', 'unknown'),\n",
    "            'events': field_info.get('events', [])\n",
    "        })\n",
    "\n",
    "print(f\"Fields matching '{SEARCH_PATTERN}': {len(matches)}\\n\")\n",
    "\n",
    "for match in matches[:20]:\n",
    "    events_str = ', '.join(match['events'][:3])\n",
    "    if len(match['events']) > 3:\n",
    "        events_str += f\" (+{len(match['events']) - 3} more)\"\n",
    "    print(f\"{match['field']} ({match['type']})\")\n",
    "    print(f\"  Found in: {events_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. RAG Knowledge Search\n",
    "\n",
    "Semantic search over indexed knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from retrieval.retrieve import search\n",
    "    \n",
    "    QUERY = \"gameStateUpdate price changes\"  # Change query\n",
    "    results = search(QUERY, top_k=5)\n",
    "    \n",
    "    print(f\"Search: '{QUERY}'\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, (doc, meta, dist) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    )):\n",
    "        print(f\"\\n[{i+1}] Distance: {dist:.3f}\")\n",
    "        print(f\"Source: {meta.get('source', 'unknown')}\")\n",
    "        print(f\"Content: {doc[:300]}...\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"RAG pipeline not available: {e}\")\n",
    "    print(\"\\nRun: cd rag-pipeline && python -m ingestion.ingest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Raw Recording Browser\n",
    "\n",
    "Browse raw WebSocket recordings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings_dir = RUGS_RECORDINGS_DIR\n",
    "\n",
    "if recordings_dir.exists():\n",
    "    recordings = list(recordings_dir.glob(\"*.jsonl\"))\n",
    "    print(f\"Found {len(recordings)} recordings in {recordings_dir}\")\n",
    "    \n",
    "    # Show recent recordings\n",
    "    recordings_sorted = sorted(recordings, key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    print(\"\\nRecent recordings:\")\n",
    "    for rec in recordings_sorted[:5]:\n",
    "        size_kb = rec.stat().st_size / 1024\n",
    "        print(f\"  {rec.name} ({size_kb:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"Recordings directory not found: {recordings_dir}\")\n",
    "    print(\"Configure RUGS_RECORDINGS_DIR in config.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Sample Recording Analysis\n",
    "\n",
    "Analyze a specific recording file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a specific recording (modify path as needed)\n",
    "if recordings_dir.exists():\n",
    "    recordings = list(recordings_dir.glob(\"*.jsonl\"))\n",
    "    if recordings:\n",
    "        sample_file = recordings[0]  # First recording\n",
    "        \n",
    "        events = []\n",
    "        with open(sample_file) as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    events.append(json.loads(line))\n",
    "        \n",
    "        print(f\"Recording: {sample_file.name}\")\n",
    "        print(f\"Total events: {len(events)}\")\n",
    "        \n",
    "        # Event type distribution\n",
    "        event_types = {}\n",
    "        for evt in events:\n",
    "            evt_type = evt.get('event', 'unknown')\n",
    "            event_types[evt_type] = event_types.get(evt_type, 0) + 1\n",
    "        \n",
    "        print(\"\\nEvent distribution:\")\n",
    "        for evt_type, count in sorted(event_types.items(), key=lambda x: -x[1]):\n",
    "            print(f\"  {evt_type}: {count}\")\n",
    "    else:\n",
    "        print(\"No recordings found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **02_canonical_review.ipynb** - Validate and promote fields to CANONICAL\n",
    "- **03_coverage_dashboard.ipynb** - Visualize documentation gaps\n",
    "- **04_rl_bot_analysis.ipynb** - Analyze RL model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Claude Flow (Python)",
   "language": "python",
   "name": "claude-flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
